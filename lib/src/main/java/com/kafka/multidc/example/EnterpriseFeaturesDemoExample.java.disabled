package com.kafka.multidc.example;

import com.kafka.multidc.KafkaMultiDatacenterClient;
import com.kafka.multidc.KafkaMultiDatacenterClientBuilder;
import com.kafka.multidc.config.*;
import com.kafka.multidc.deadletter.DefaultDeadLetterConfig;
import com.kafka.multidc.deadletter.DeadLetterQueueHandler;
import com.kafka.multidc.resilience.ResilienceConfiguration;
import com.kafka.multidc.routing.RoutingStrategy;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.header.internals.RecordHeaders;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.time.Duration;
import java.util.List;
import java.util.Map;

/**
 * Comprehensive example demonstrating all enterprise features of the Kafka Multi-Datacenter Client.
 * 
 * This example showcases:
 * - Advanced security configuration with SSL/TLS and SASL
 * - Schema registry integration with health monitoring
 * - Resilience patterns (circuit breaker, retry, rate limiter)
 * - Dead letter queue handling
 * - Transaction support
 * - Consumer group management
 * - Advanced observability and metrics
 * - Performance optimization
 * - Multi-format serialization with compression
 */
public class EnterpriseFeaturesDemoExample {
    
    private static final Logger logger = LoggerFactory.getLogger(EnterpriseFeaturesDemoExample.class);
    
    public static void main(String[] args) {
        logger.info("=== Kafka Multi-Datacenter Client Enterprise Features Demo ===");
        
        try {
            // Create enterprise-grade configuration
            KafkaDatacenterConfiguration config = createEnterpriseConfiguration();
            
            // Build the client with all enterprise features enabled
            try (KafkaMultiDatacenterClient client = KafkaMultiDatacenterClientBuilder.create()
                .configuration(config)
                .build()) {
                
                logger.info("Enterprise client created successfully");
                
                // Demonstrate each enterprise feature
                demonstrateAdvancedSecurity(client);
                demonstrateSchemaRegistry(client);
                demonstrateResiliencePatterns(client);
                demonstrateDeadLetterQueue(client);
                demonstrateTransactions(client);
                demonstrateConsumerGroupManagement(client);
                demonstrateObservabilityFeatures(client);
                demonstratePerformanceOptimization(client);
                demonstrateAdvancedSerialization(client);
                
                logger.info("All enterprise features demonstrated successfully");
            }
            
        } catch (Exception e) {
            logger.error("Enterprise demo failed", e);
        }
    }
    
    /**
     * Create comprehensive enterprise configuration with all features enabled.
     */
    private static KafkaDatacenterConfiguration createEnterpriseConfiguration() {
        logger.info("Creating enterprise configuration...");
        
        // Configure advanced security
        SecurityConfig securityConfig = SecurityConfig.builder()
            .enableSsl()
            .sslProtocol("TLSv1.3")
            .keystore("/path/to/enterprise.keystore.jks", "enterprise-keystore-password")
            .truststore("/path/to/enterprise.truststore.jks", "enterprise-truststore-password")
            .saslScram512("enterprise-kafka-client", "enterprise-secure-password")
            .clientAuthRequired()
            .build();
        
        // Configure schema registry with multi-datacenter support
        SchemaRegistryConfig schemaConfig = SchemaRegistryConfig.builder()
            .url("https://schema-registry-east.enterprise.com")
            .addFallbackUrl("https://schema-registry-west.enterprise.com")
            .username("schema-registry-user")
            .password("schema-registry-password")
            .connectionTimeout(Duration.ofSeconds(30))
            .readTimeout(Duration.ofSeconds(60))
            .maxRetries(3)
            .retryBackoffMs(1000)
            .cacheEnabled(true)
            .cacheMaxSize(1000)
            .cacheTtl(Duration.ofMinutes(30))
            .healthCheckEnabled(true)
            .healthCheckInterval(Duration.ofSeconds(30))
            .enableSsl(true)
            .bearerToken("schema-registry-bearer-token")
            .build();
        
        // Configure resilience patterns
        ResilienceConfig resilienceConfig = ResilienceConfig.builder()
            .circuitBreakerEnabled(true)
            .circuitBreakerFailureRateThreshold(50.0f)
            .circuitBreakerWaitDurationInOpenState(Duration.ofSeconds(30))
            .circuitBreakerSlidingWindowSize(10)
            .retryEnabled(true)
            .retryMaxAttempts(3)
            .retryWaitDuration(Duration.ofSeconds(2))
            .rateLimiterEnabled(true)
            .rateLimiterLimitForPeriod(100)
            .rateLimiterLimitRefreshPeriod(Duration.ofSeconds(1))
            .bulkheadEnabled(true)
            .bulkheadMaxConcurrentCalls(25)
            .bulkheadMaxWaitDuration(Duration.ofSeconds(5))
            .timeLimiterEnabled(true)
            .timeLimiterTimeoutDuration(Duration.ofSeconds(10))
            .build();
        
        // Configure Dead Letter Queue
        DeadLetterQueueHandler.DeadLetterConfig dlqConfig = DefaultDeadLetterConfig.builder()
            .deadLetterTopicSuffix(".enterprise.dlq")
            .maxRetryAttempts(5)
            .enabled(true)
            .strategy(DeadLetterQueueHandler.DeadLetterStrategy.TOPIC_BASED)
            .retryBackoffMultiplier(2.0)
            .maxRetryDelay(Duration.ofMinutes(5))
            .enableMetrics(true)
            .build();
        
        // Configure performance optimization
        return KafkaDatacenterConfiguration.builder()
            .datacenters(List.of(
                KafkaDatacenterEndpoint.builder()
                    .id("enterprise-east")
                    .region("us-east-1")
                    .bootstrapServers("kafka-east-1.enterprise.com:9092,kafka-east-2.enterprise.com:9092")
                    .priority(1)
                    .compressionType("lz4")
                    .enableIdempotence(true)
                    .batchSize(65536)  // 64KB batches for high throughput
                    .lingerMs(100)     // Wait up to 100ms for batching
                    .bufferMemory(134217728)  // 128MB buffer
                    .maxInFlightRequestsPerConnection(5)
                    .securityConfig(securityConfig)
                    .build(),
                KafkaDatacenterEndpoint.builder()
                    .id("enterprise-west")
                    .region("us-west-2")
                    .bootstrapServers("kafka-west-1.enterprise.com:9092,kafka-west-2.enterprise.com:9092")
                    .priority(2)
                    .compressionType("lz4")
                    .enableIdempotence(true)
                    .batchSize(65536)
                    .lingerMs(100)
                    .bufferMemory(134217728)
                    .maxInFlightRequestsPerConnection(5)
                    .securityConfig(securityConfig)
                    .build()
            ))
            .localDatacenter("enterprise-east")
            .routingStrategy(RoutingStrategy.HEALTH_AWARE)
            .healthCheckInterval(Duration.ofSeconds(15))
            .connectionTimeout(Duration.ofSeconds(30))
            .requestTimeout(Duration.ofMinutes(2))
            .enableMetrics(true)
            .metricsPrefix("enterprise.kafka.multidc")
            .schemaRegistryConfig(schemaConfig)
            .resilienceConfig(resilienceConfig)
            .deadLetterConfig(dlqConfig)
            .build();
    }
    
    /**
     * Demonstrate advanced security features including SSL/TLS and SASL authentication.
     */
    private static void demonstrateAdvancedSecurity(KafkaMultiDatacenterClient client) {
        logger.info("=== Advanced Security Demo ===");
        
        try {
            // Create a secure message with headers
            RecordHeaders secureHeaders = new RecordHeaders();
            secureHeaders.add("encryption-level", "AES-256".getBytes());
            secureHeaders.add("authentication-method", "SASL-SCRAM-SHA-512".getBytes());
            secureHeaders.add("data-classification", "confidential".getBytes());
            
            ProducerRecord<String, String> secureRecord = new ProducerRecord<>(
                "enterprise-secure-events",
                null, // partition will be auto-assigned
                "enterprise-user-123",
                "{\"userId\":\"enterprise-user-123\",\"action\":\"secure-transaction\",\"amount\":10000.50,\"timestamp\":\"" + System.currentTimeMillis() + "\"}",
                secureHeaders
            );
            
            // Send with enterprise security
            var metadata = client.producerSync().send(secureRecord);
            logger.info("‚úÖ Secure message sent successfully to partition {} at offset {}", 
                       metadata.partition(), metadata.offset());
            
            // Demonstrate secure async sending
            client.producerAsync().sendAsync(secureRecord)
                .thenAccept(meta -> logger.info("‚úÖ Async secure message sent to partition {} at offset {}", 
                           meta.partition(), meta.offset()))
                .exceptionally(error -> {
                    logger.error("‚ùå Async secure send failed", error);
                    return null;
                });
            
        } catch (Exception e) {
            logger.error("‚ùå Security demonstration failed", e);
        }
    }
    
    /**
     * Demonstrate schema registry integration with validation and evolution.
     */
    private static void demonstrateSchemaRegistry(KafkaMultiDatacenterClient client) {
        logger.info("=== Schema Registry Demo ===");
        
        try {
            // Create schema-compliant enterprise user event
            Map<String, Object> enterpriseUserEvent = Map.of(
                "userId", "enterprise-user-456",
                "eventType", "ENTERPRISE_LOGIN",
                "timestamp", System.currentTimeMillis(),
                "metadata", Map.of(
                    "source", "enterprise-sso",
                    "version", "3.0",
                    "security-level", "HIGH",
                    "compliance-flags", List.of("SOX", "GDPR", "HIPAA")
                ),
                "userDetails", Map.of(
                    "department", "Finance",
                    "role", "Director",
                    "clearanceLevel", "SECRET"
                )
            );
            
            // Send with schema validation
            ProducerRecord<String, Map<String, Object>> userEventRecord = new ProducerRecord<>(
                "enterprise-user-events", 
                "enterprise-user-456", 
                enterpriseUserEvent
            );
            var metadata = client.producerSync().send(userEventRecord);
            logger.info("‚úÖ Schema-validated enterprise event sent to partition {} at offset {}", 
                       metadata.partition(), metadata.offset());
            
            // Demonstrate schema evolution compatibility check
            Map<String, Object> evolvedEvent = Map.of(
                "userId", "enterprise-user-789",
                "eventType", "ENTERPRISE_LOGIN_V2",
                "timestamp", System.currentTimeMillis(),
                "metadata", Map.of(
                    "source", "enterprise-sso",
                    "version", "4.0",  // Schema evolution
                    "security-level", "MAXIMUM",
                    "compliance-flags", List.of("SOX", "GDPR", "HIPAA", "PCI-DSS")
                ),
                "userDetails", Map.of(
                    "department", "Finance",
                    "role", "Director",
                    "clearanceLevel", "TOP_SECRET"
                ),
                "additionalData", Map.of(  // New field in evolved schema
                    "loginMethod", "MFA",
                    "deviceFingerprint", "enterprise-device-123"
                )
            );
            
            ProducerRecord<String, Map<String, Object>> evolvedEventRecord = new ProducerRecord<>(
                "enterprise-user-events", 
                "enterprise-user-789", 
                evolvedEvent
            );
            var evolvedMetadata = client.producerSync().send(evolvedEventRecord);
            logger.info("‚úÖ Schema-evolved enterprise event sent to partition {} at offset {}", 
                       evolvedMetadata.partition(), evolvedMetadata.offset());
            
        } catch (Exception e) {
            logger.error("‚ùå Schema registry demonstration failed", e);
        }
    }
    
    /**
     * Demonstrate resilience patterns including circuit breaker, retry, and rate limiting.
     */
    private static void demonstrateResiliencePatterns(KafkaMultiDatacenterClient client) {
        logger.info("=== Resilience Patterns Demo ===");
        
        try {
            // Demonstrate rate limiting by sending multiple messages rapidly
            logger.info("Testing rate limiter with burst of messages...");
            for (int i = 0; i < 50; i++) {
                try {
                    ProducerRecord<String, String> record = new ProducerRecord<>(
                        "enterprise-high-volume",
                        "key-" + i,
                        "{\"messageId\":" + i + ",\"content\":\"high-volume-test\",\"timestamp\":\"" + System.currentTimeMillis() + "\"}"
                    );
                    
                    client.producerSync().send(record);
                    
                    if (i % 10 == 0) {
                        logger.info("Sent {} messages (rate limiter active)", i + 1);
                    }
                } catch (Exception e) {
                    logger.warn("Rate limiter throttled message {}: {}", i, e.getMessage());
                }
            }
            
            // Demonstrate circuit breaker resilience
            logger.info("Testing circuit breaker with deliberate failures...");
            for (int i = 0; i < 15; i++) {
                try {
                    // Send to a topic that might cause failures to trigger circuit breaker
                    ProducerRecord<String, String> record = new ProducerRecord<>(
                        "enterprise-circuit-test",
                        "circuit-test-" + i,
                        "{\"test\":\"circuit-breaker\",\"attempt\":" + i + "}"
                    );
                    
                    client.producerSync().send(record);
                    logger.info("Circuit breaker test message {} sent successfully", i);
                    
                } catch (Exception e) {
                    logger.warn("Circuit breaker triggered for message {}: {}", i, e.getMessage());
                    if (i > 10) {
                        logger.info("Circuit breaker is likely OPEN, stopping test");
                        break;
                    }
                }
            }
            
            // Wait for circuit breaker to potentially close
            Thread.sleep(5000);
            logger.info("Circuit breaker should be transitioning back to CLOSED state");
            
        } catch (Exception e) {
            logger.error("‚ùå Resilience patterns demonstration failed", e);
        }
    }
    
    /**
     * Demonstrate Dead Letter Queue handling for failed messages.
     */
    private static void demonstrateDeadLetterQueue(KafkaMultiDatacenterClient client) {
        logger.info("=== Dead Letter Queue Demo ===");
        
        try {
            // Send some messages that might fail processing
            for (int i = 0; i < 5; i++) {
                ProducerRecord<String, String> record = new ProducerRecord<>(
                    "enterprise-dlq-test",
                    "dlq-test-" + i,
                    "{\"messageId\":" + i + ",\"shouldFail\":" + (i % 2 == 0) + ",\"content\":\"DLQ test message\"}"
                );
                
                try {
                    var metadata = client.producerSync().send(record);
                    logger.info("DLQ test message {} sent to partition {} at offset {}", 
                               i, metadata.partition(), metadata.offset());
                } catch (Exception e) {
                    logger.warn("Message {} failed and should be routed to DLQ: {}", i, e.getMessage());
                }
            }
            
            // Simulate consumer processing that might send messages to DLQ
            logger.info("DLQ processing simulation completed - check DLQ topics for failed messages");
            
        } catch (Exception e) {
            logger.error("‚ùå Dead Letter Queue demonstration failed", e);
        }
    }
    
    /**
     * Demonstrate transactional operations for exactly-once semantics.
     */
    private static void demonstrateTransactions(KafkaMultiDatacenterClient client) {
        logger.info("=== Transactions Demo ===");
        
        try {
            // NOTE: Transaction operations would be used when available
            logger.info("Transaction operations would provide exactly-once semantics");
            logger.info("Features would include:");
            logger.info("- client.getTransactionOperations().executeInTransaction()");
            logger.info("- client.getTransactionOperations().executeInTransactionAsync()");
            logger.info("- Atomic multi-datacenter commits");
            logger.info("- Cross-datacenter transaction coordination");
            
            // Simulate transaction-like behavior using regular sends
            logger.info("Simulating enterprise transaction...");
            
            // Financial transaction: debit from one account, credit to another
            ProducerRecord<String, String> debitRecord = new ProducerRecord<>(
                "enterprise-accounts", "account-123", 
                "{\"action\":\"DEBIT\",\"amount\":50000.00,\"currency\":\"USD\",\"timestamp\":\"" + System.currentTimeMillis() + "\"}"
            );
            client.producerSync().send(debitRecord);
            
            ProducerRecord<String, String> creditRecord = new ProducerRecord<>(
                "enterprise-accounts", "account-456", 
                "{\"action\":\"CREDIT\",\"amount\":50000.00,\"currency\":\"USD\",\"timestamp\":\"" + System.currentTimeMillis() + "\"}"
            );
            client.producerSync().send(creditRecord);
                
            // Audit trail
            ProducerRecord<String, String> auditRecord = new ProducerRecord<>(
                "enterprise-audit", "transaction-" + System.currentTimeMillis(), 
                "{\"type\":\"TRANSFER\",\"from\":\"account-123\",\"to\":\"account-456\",\"amount\":50000.00,\"status\":\"COMPLETED\"}"
            );
            client.producerSync().send(auditRecord);
                
            // Compliance reporting
            ProducerRecord<String, String> complianceRecord = new ProducerRecord<>(
                "enterprise-compliance", "compliance-" + System.currentTimeMillis(), 
                "{\"transactionType\":\"LARGE_TRANSFER\",\"amount\":50000.00,\"requiresReview\":true}"
            );
            client.producerSync().send(complianceRecord);
                
            logger.info("‚úÖ Enterprise transaction simulation completed successfully!");
            
            // NOTE: Async transaction operations would work similarly when available
            logger.info("Async transaction operations would provide non-blocking exactly-once semantics");
                logger.info("Starting async enterprise transaction...");
                
                client.producerSync().send("enterprise-orders", "order-" + System.currentTimeMillis(), 
                    "{\"productId\":\"ENTERPRISE-LICENSE\",\"quantity\":100,\"price\":500000.00}");
                
                client.producerSync().send("enterprise-inventory", "inventory-update-" + System.currentTimeMillis(), 
                    "{\"productId\":\"ENTERPRISE-LICENSE\",\"quantityChange\":-100}");
                
                logger.info("‚úÖ Async enterprise transaction completed!");
                return null;
            }).thenAccept(result -> logger.info("Async transaction callback executed"))
              .exceptionally(error -> {
                  logger.error("‚ùå Async transaction failed", error);
                  return null;
              });
            
        } catch (Exception e) {
            logger.error("‚ùå Transaction demonstration failed", e);
        }
    }
    
    /**
     * Demonstrate consumer group management features.
     */
    private static void demonstrateConsumerGroupManagement(KafkaMultiDatacenterClient client) {
        logger.info("=== Consumer Group Management Demo ===");
        
        try {
            // Get consumer group manager
            var groupManager = client.getConsumerGroupManager();
            
            // List existing consumer groups
            var groups = groupManager.listConsumerGroups();
            logger.info("Found {} consumer groups", groups.size());
            groups.forEach(group -> logger.info("Consumer group: {} (state: {})", 
                          group.getGroupId(), group.getState()));
            
            // Get detailed information about a specific group
            if (!groups.isEmpty()) {
                var firstGroup = groups.get(0);
                var groupDescription = groupManager.describeConsumerGroup(firstGroup.getGroupId());
                logger.info("Group {} has {} members", 
                           groupDescription.getGroupId(), 
                           groupDescription.getMembers().size());
            }
            
            // Demonstrate consumer group metrics
            var metrics = groupManager.getConsumerGroupMetrics("enterprise-consumer-group");
            logger.info("Consumer group metrics retrieved for monitoring");
            
        } catch (Exception e) {
            logger.error("‚ùå Consumer group management demonstration failed", e);
        }
    }
    
    /**
     * Demonstrate observability features including metrics and health monitoring.
     */
    private static void demonstrateObservabilityFeatures(KafkaMultiDatacenterClient client) {
        logger.info("=== Observability Features Demo ===");
        
        try {
            // Monitor datacenter health changes
            client.subscribeToHealthChanges((datacenter, healthy) -> {
                logger.info("üîî Health Alert: Datacenter {} is now {}", 
                           datacenter.getId(), healthy ? "HEALTHY" : "UNHEALTHY");
            });
            
            // Get current health status
            var healthStatus = client.getHealthStatus();
            logger.info("Current health status:");
            healthStatus.forEach((datacenter, health) -> 
                logger.info("  {} - {} (last check: {})", 
                           datacenter.getId(), 
                           health.isHealthy() ? "HEALTHY" : "UNHEALTHY",
                           health.getLastCheckTime()));
            
            // Demonstrate custom metrics
            logger.info("Enterprise metrics are being collected and exposed via Micrometer");
            logger.info("Metrics include: message throughput, error rates, latency percentiles, connection pool status");
            
        } catch (Exception e) {
            logger.error("‚ùå Observability demonstration failed", e);
        }
    }
    
    /**
     * Demonstrate performance optimization features.
     */
    private static void demonstratePerformanceOptimization(KafkaMultiDatacenterClient client) {
        logger.info("=== Performance Optimization Demo ===");
        
        try {
            // Send high-throughput batch of messages to test performance optimizations
            long startTime = System.currentTimeMillis();
            int messageCount = 1000;
            
            logger.info("Sending {} messages to test performance optimizations...", messageCount);
            
            for (int i = 0; i < messageCount; i++) {
                ProducerRecord<String, String> record = new ProducerRecord<>(
                    "enterprise-performance-test",
                    "perf-key-" + i,
                    "{\"messageId\":" + i + ",\"payload\":\"" + "x".repeat(1024) + "\",\"timestamp\":\"" + System.currentTimeMillis() + "\"}"
                );
                
                // Use async for better throughput
                client.producerAsync().send(record);
                
                if (i % 100 == 0 && i > 0) {
                    logger.info("Sent {} messages...", i);
                }
            }
            
            long endTime = System.currentTimeMillis();
            long duration = endTime - startTime;
            double throughput = (messageCount * 1000.0) / duration;
            
            logger.info("‚úÖ Performance test completed: {} messages in {}ms ({:.2f} msg/sec)", 
                       messageCount, duration, throughput);
            logger.info("Performance optimizations active: batching, compression, connection pooling");
            
        } catch (Exception e) {
            logger.error("‚ùå Performance optimization demonstration failed", e);
        }
    }
    
    /**
     * Demonstrate advanced serialization with compression and encryption.
     */
    private static void demonstrateAdvancedSerialization(KafkaMultiDatacenterClient client) {
        logger.info("=== Advanced Serialization Demo ===");
        
        try {
            // Create complex enterprise data structure
            Map<String, Object> enterpriseData = Map.of(
                "transactionId", "TXN-" + System.currentTimeMillis(),
                "customerData", Map.of(
                    "customerId", "ENTERPRISE-CUSTOMER-123",
                    "name", "Enterprise Corp",
                    "tier", "PLATINUM",
                    "contracts", List.of("CONTRACT-A", "CONTRACT-B", "CONTRACT-C")
                ),
                "financialData", Map.of(
                    "amount", 1000000.00,
                    "currency", "USD",
                    "exchangeRate", 1.0,
                    "accountDetails", Map.of(
                        "routingNumber", "123456789",
                        "accountNumber", "XXXX-XXXX-XXXX-1234"
                    )
                ),
                "metadata", Map.of(
                    "compression", "LZ4",
                    "encryption", "AES-256",
                    "serialization", "JSON_WITH_SCHEMA"
                )
            );
            
            // Send with advanced serialization (compression + encryption automatically applied)
            var metadata = client.producerSync().send("enterprise-serialization-test", 
                                                     "enterprise-data-" + System.currentTimeMillis(), 
                                                     enterpriseData);
            
            logger.info("‚úÖ Advanced serialized data sent to partition {} at offset {}", 
                       metadata.partition(), metadata.offset());
            logger.info("Data automatically compressed with LZ4 and encrypted with AES-256");
            
        } catch (Exception e) {
            logger.error("‚ùå Advanced serialization demonstration failed", e);
        }
    }
}
